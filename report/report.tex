\documentclass{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{framed}

% For including other sections 
\usepackage{standalone}

%% Simple AMSthm environments, numbered together.
\newtheorem{lem}{Lemma}
\newtheorem{thm}[lem]{Theorem}
\newtheorem{conj}[lem]{Conjecture}

\theoremstyle{definition}
\newtheorem{defn}[lem]{Definition}

% Various mathematical helper definitions
\newcommand{\Z}{\mathbb{Z}}



\title{Project 2 Report}
\begin{document}
\maketitle

\section{Overview}
\begin{framed}
  Discuss the high level goals of your work, along with any
  interesting/key findings.
\end{framed}

Our objective was to understand and experience the issues that arise
when actually implementing program analyses. To this end, we
constructed a dataflow analysis framework based on the LLVM compiler
infastructure.

We confirmed through experience that SSA makes flow functions
\emph{much} easier to write. We found that the most helpful part of
the LLVM API is the object hierarchy for dealing with the graph of
Value objects representing LLVM IR. Subclasses of Value are rich,
representing functions, basic blocks, instructions, constants,
etc. These IR object classes provide many useful convienence functions
and make it easy to traverse the program. The least helpful parts of
the LLVM API is the pass manager and the special LLVM replacement for
C++ run-time type information; these features are obscure and
difficult to use.

The most surprising feature of LLVM was the InstVisitor template
class. This class made our implementation much easier and cleaner than
it would have been otherwise, but examples in the introductory
documentation do not use it. For something so helpful to LLVM
newcomers, InstVisitor is poorly advertised in the LLVM documentation.

Though the analyses were relatively straightforward in terms of flow
functions, the technical hurdles of actually making them work were
nontrivial, and we learned a great deal. This project provided a solid
foundation of practical experience for building programming-language
tools based on LLVM. We document our implementation efforts in the
following pages, beginning with the overall design for the dataflow
analysis framework. 

% We found numerous conceptual mismatches between the LLVM IR and other
% programming languages (including assemblers). Registers may only be
% assigned to at one site of the program (SSA), and variables in a
% high-level language may be represented by either a sequence of these
% registers or as a memory location. The expressions of a high-level
% language can become chains of instructions in the IR, changing the
% definition of CSE. The reasons for these design choices became clear
% as we worked our way through implementation of the analyses, and will
% be expanded on in the appropriate sections.


\section{Interface Design}
\begin{framed}
  Describe interface. Discuss what alternative designs you may have
  also considered, and explain the tradeoffs that ultimately led to
  your choice.
\end{framed}


\begin{description}
\item[LatticePoint] Every dataflow analysis requires a Lattice to work
  over. The \emph{LatticePoint} class describes our interface to these
  points. Anything that inherits from \emph{LatticePoint} must
  implement:
  \begin{description}
  \item[equals$(LP) \rightarrow $Bool] Compare this latticepoint and
    the argument for equality, returning a Boolean.
  \item[join$(LP) \rightarrow LP$] Return a new LatticePoint
    containing the join of this LatticePoint and the argument.
  \item[isBottom$() \rightarrow $Bool] Returns true if this
    latticePoint is Bottom
  \item[isTop$() \rightarrow $Bool] Returns true if this latticePoint
    is Top.
  \end{description}
\item[FlowFunction] This class provides an interface to calling flow
  functions. Derived classes must implement a single special function:
  \[
  \textbf{operator}(Instruction, Vector \langle LP\rangle) \rightarrow
  Vector \langle LP \rangle
  \]
  Implementing the ``operator'' function makes objects of this type
  callable. To define each analysis we make a subclass of
  \emph{FlowFunction} that can operate on a particular type of
  \emph{LatticePoint}. When ``operator'' is called, a single step of
  the flow function is exectued with the argument \emph{LatticePoints}
  as incoming edges on the control flow graph. A vector of
  \emph{LatticePoints} that should be on the outgoing edges of this
  instruction is returned.
\item[Analysis] This class contains the implementation of the worklist
  algorithm. It is not an interface, and is not meant to be derived
  from. It has one public function:
  \[
  \textbf{analyze}(Function, FlowFunction, LP) \rightarrow
  Map \langle Instruction, LP \rangle
  \]
\end{description}

To use our framework, a caller  first instanciates a ``bottom''
\emph{LatticePoint} and a \emph{FlowFunction} of matching types. Then,
in an LLVM Pass, the caller supplies a function and the aforementioned
objects to \textbf{analyze}. 

Adding an additional analysis to the framework amounts to providing
new derived classes of \emph{LatticePoint} and \emph{FlowFunction}.

% issue with this design: positional encoding of true/false branches
% (or whatever) in the return type of operator()

Unsure how honest to be here. Tradeoffs ended up being, how many C++
features can we avoid and still have a functioning C++ program? Other
than this issue, describing our interface (flowfunction object,
latticepoint object, and a blob of code implementing the worklist
algorithm that uses runtime polymorphism to use these objects) is
pretty straightforward. I (marco) can write this.

\section{Analyses}

\subsection{Constant Propagation}
\input{CPsection}

\subsection{Available Expressions}
\input{CSEsection}

\subsection{Range Analysis}
\input{RAsection}


\subsection{Intra-Procedural Pointer Analysis}
\input{PAsection}


\section{Testing}
\begin{framed}
  Make sure to explain assumptions you make about the code you
  analyze. For instance, for pointer analysis, you may have made some
  assumptions about the aliasing information known about input
  parameters. Explain those assumptions and why they are reasonable.

  Part of this project is to come up with a useful set of benchmarks
  on which to test and improve your analysis. Discuss why you chose
  those benchmarks, and what makes them interesting. If your
  implementation fails on some benchmarks (there's no shame in it!),
  then explain why and how the analysis might be improved.
\end{framed}
\subsection{Benchmarks/Assumptions in Common}
Because we have a common pool of benchmarks, we can list them here
along with common assumptions on code. More specialized discussion of
per-analysis benchmark goes below. Here, we can also introduce the
Straight Line Program/Branching Program distinction.

\subsection{Constant Propagation}
\subsection{Available Expressions}
\subsection{Range Analysis}
\subsection{Intra-Procedural Pointer Analysis}



\section{Conclusion/Challenges}
\begin{framed}
  As this project is significantly more exploratory than the first, we
  want you to tell us what you found particularly
  interesting/challenging/frustrating. What extensions to the project
  did you attempt? (for instance, did you try combining the results of
  analyses, or did you try your hand at interprocedural
  analysis?). What aspects of LLVM made it easy/hard to implement your
  design? If you could redo your project with what you know now, what
  changes would you make?
\end{framed}
Also unsure how honest to be here. Most of our challenges had to do
with counter-intuitive LLVM design and poor documentation, as well as
the unpredictability of C++ features.

\end{document}

